# 10 集成学习 #
---
若我们以民主选举过程来比拟分类器的运作过程，则选举中每位选民的一次投票相当于一个基分类器的分类结果，而一大批选民的一次投票则可以认为是一个集成分类算法的分类结果。可以想象，如果选举仅由一个人的投票来决定，那么选举结果的稳定性、可靠性都是很值得怀疑的，因为如果换一个人来投票，或在同一个人的不同情绪状况下投票，结果将会有很大差异，正如一个基分类器的分类结果往往难以令人满意；而若是由全民投票来产生结果，则这一结果可以提现大多数人的意志，就算多举行几次投票，结果也将稳定，且可以认为该结果是正确的。这正是将基分类器进行集成的目的所在－－使分类结果稳定，且正确率高。

##10.1 概述##
###10.1.1 一个概率论小计算###
首先，我们从一个简单的概率论小计算引入，来说明"集成"的功效:设共有n个基分类器，每个基分类器的预测正确率都为0.5，即一半的正确率，相当于乱猜;但当我们考虑用这n个基分类器共同进行预测，该预测结果正确的概率P等于"1-n个分类器全部预测错误的概率"，即P=1-(1-0.5)^n。
在n=5时，P=0.96875；n=15时，P=0.99997;n=25时，P=1.00000.也就是说，25个"乱猜"的基分类器预测结果"集成"后，其预测正确率趋近于1，这就是"集成算法"的基本思想和神奇所在。
### 10.1.2 Bagging算法 ###
Bagging是Bootstrap Aggregating的缩写，简单来说，就是通过使用bootstrap抽样(bootstrap，自助抽样法是一种从给定训练集中等概率、有放回地进行重复抽样，也就是说，每当选中一个样本，它等可能地被再次选中并被再次添加到训练集中)得到若干不同的训练集，以这些训练集分别建立模型，即得到一系列基分类器，这些分类器由于来自不同的训练样本，他们对同一测试集的预测效果不一。因此，Bagging算法随后对基分类器的一系列预测结果进行投票(分类问题)或平均(回归问题)，从而得到每一个测试集样本的最终预测结果，这一集成后的结果往往是准确而稳定的。
比如现有基分类器1到10，它们对某样本的预测结果分别为类别1、2、1、1、1、1、2、1、1、2，则Bagging给出的最终结果即为"该样本属于类别1"，因为大多数基分类器将票投给了类别1.

### 10.1.3 AdaBoost算法 ###
AdaBoost相对于Bagging算法更为巧妙，且一般来说是效果更优的集成分类算法，尤其在数据集分布不平衡的情况下，其优势更为显著。该算法的提出先于Bagging，但在复杂度和效果上高于Bagging，因此考虑先行介绍Bagging算法。
AdaBoost同样是在若干基分类器基础上的一种集成算法，但不同于Bagging对一系列预测结果的简单综合，该算法在依次构建基分类器的过程中，会根据上一个基分类器对各训练集样本的预测结果，自行调整在本次基分类器构造时，各样本被抽中的概率。具体来说，如果在上一基分类器的预测中，样本i被错误分类了，那么在这一次基分类器的训练样本抽取过程中，样本i就会被赋予较高的权重，以使其能够以较大的可能被抽中，从而提高其被正确分类的概率。
这样一个实时调节权重的过程正是AdaBoost算法的优势所在，它通过将若干具有互补性质的基分类器集合于一体，显著提高了集成分类器的稳定性和准确性。另外，Bagging和AdaBoost的基分类器的选取都是任意的，但绝大多数我们使用决策树，因为决策树可以同时处理数值、类别、次序等各类型变量，且变量的选择也较容易。